<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>用户案例 on Apache Dubbo</title><link>https://cn.dubbo.apache.org/zh-cn/blog/users/</link><description>Recent content in 用户案例 on Apache Dubbo</description><generator>Hugo</generator><language>zh-cn</language><atom:link href="https://cn.dubbo.apache.org/zh-cn/blog/users/index.xml" rel="self" type="application/rss+xml"/><item><title>阿里巴巴升级 Dubbo3 全面取代 HSF2</title><link>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/16/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E5%8D%87%E7%BA%A7-dubbo3-%E5%85%A8%E9%9D%A2%E5%8F%96%E4%BB%A3-hsf2/</link><pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate><guid>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/16/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E5%8D%87%E7%BA%A7-dubbo3-%E5%85%A8%E9%9D%A2%E5%8F%96%E4%BB%A3-hsf2/</guid><description>&lt;p>继业务全面上云后，2022 双十一，阿里巴巴微服务技术栈全面迁移到以 Dubbo3 为代表的云上开源标准中间件体系。在业务上，基于 Dubbo3 首次实现了关键业务不停推、不降级的全面用户体验提升，从技术上，大幅提高研发与运维效率的同时地址推送等关键资源利用率提升超 40%，基于三位一体的 Dubbo3 开源中间件体系打造了阿里在云上的单元化最佳实践和统一标准，同时将规模化实践经验与技术创新贡献开源社区，成为微服务开源技术与标准发展的核心源泉与推动力。&lt;/p>
&lt;h2 id="1-阿里电商">1 阿里电商&lt;/h2>
&lt;p>&lt;img alt="image.png" src="https://cn.dubbo.apache.org/imgs/blog/users/tmall.png"> &lt;img alt="image.png" src="https://cn.dubbo.apache.org/imgs/blog/users/taobao.png"> &lt;img alt="image.png" src="https://cn.dubbo.apache.org/imgs/blog/users/kaola.png"> &amp;hellip;&amp;hellip;&lt;/p>
&lt;p>整个电商体系的所有核心应用，包括交易相关、导购相关都都升级到了 Dubbo3 体系，用来升级原有的 HSF 框架，阿里电商是对 Dubbo3 实践最广泛、需求最强烈的体系，基于 Dubbo3 实现了以下关键目标。
2022 618大促、双11 大促期间 超 2000+ 应用、40w 节点均跑在 Dubbo3 之上。&lt;/p>
&lt;ul>
&lt;li>应用级服务发现，解决了大促期间地址推送降级的问题，部分关键链路提升单机资源利用率 40%，大促期间地址推送SLA保障、资源利用率。&lt;/li>
&lt;li>Triple协议，解决跨网关高效互通的问题，同时部分业务线升级了 Streaming 编程和通信模式。&lt;/li>
&lt;li>统一的流量治理规则，阿里电商场景的路由规则非常复杂，基于实现了云原生体系的结合。&lt;/li>
&lt;li>Service Mesh 解决方案，Thin SDK + Proxyless 的解决方案&lt;/li>
&lt;li>服务柔性，目前正在落地和探索自适应负载均衡、自适应限流等策略。&lt;/li>
&lt;/ul>
&lt;h2 id="2-蚂蚁金服">2 蚂蚁金服&lt;/h2>
&lt;p>&lt;img alt="image.png" src="https://cn.dubbo.apache.org/imgs/blog/users/antpay.png"> &lt;img alt="image.png" src="https://cn.dubbo.apache.org/imgs/blog/users/feizhu.png"> &lt;img alt="image.png" src="https://cn.dubbo.apache.org/imgs/blog/users/1688.png">&lt;img alt="image.png" src="https://cn.dubbo.apache.org/imgs/blog/users/taobao.png">&lt;/p>
&lt;p>阿里集团内与蚂蚁体系的互通，目前都跑在 Dubbo3 Triple 互通链路上，与原有基于 HSF 的互通方案对比，Triple 协议链路的 RT 降低了 50%。集团与蚂蚁东西向流量的核心链路，飞猪、手淘、口碑、饿了么、1688、部分导购应用、商品库、评价等业务都采用此方案。&lt;/p>
&lt;h2 id="3-本地生活">3 本地生活&lt;/h2>
&lt;p>&lt;img alt="image.png" src="https://cn.dubbo.apache.org/imgs/blog/users/eleme.png"> &lt;img alt="image.png" src="https://cn.dubbo.apache.org/imgs/blog/users/amap.png">&lt;/p>
&lt;p>截止 2022 年初，Dubbo3 实现了在饿了么全量业务的生产上线，取代了之前自建的微服务体系，在过去的近一年时间内，饿了么线上有 2000 应用、15w 实例节点平稳跑在 Dubbo3 之上。&lt;/p></description></item><item><title>工商银行 Dubbo3 应用级服务发现实践</title><link>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E5%B7%A5%E5%95%86%E9%93%B6%E8%A1%8C-dubbo3-%E5%BA%94%E7%94%A8%E7%BA%A7%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E5%AE%9E%E8%B7%B5/</link><pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate><guid>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E5%B7%A5%E5%95%86%E9%93%B6%E8%A1%8C-dubbo3-%E5%BA%94%E7%94%A8%E7%BA%A7%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%E5%AE%9E%E8%B7%B5/</guid><description>&lt;h3 id="问题分析">问题分析&lt;/h3>
&lt;p>以下是经典的 Dubbo 的工作原理图，服务提供者和消费者通过注册中心协调实现地址的自动发现。&lt;/p>
&lt;p>&lt;img alt="icbc-analyze" src="https://cn.dubbo.apache.org/imgs/user/icbc/icbc-analyze.png">&lt;/p>
&lt;p>工商银行面临的主要瓶颈是在注册中心与服务消费端，接口级别地址的数量已经是亿级规模，一方面存储容量达到瓶颈、另一方面推送效率明显下降；而在消费端这一侧，Dubbo2 框架常驻内存已超 40%，每次地址推送带来的 cpu 等资源消耗率也非常高，影响正常的业务调用。&lt;/p>
&lt;p>这是 Dubbo2 接口级服务发现架构在大规模集群场景下的固有问题（具体原因请查看应用级服务发现原理解析），通过常规的性能优化无法从根本上解决问题。因此工商银行采用了 Dubbo3 中提出的应用级服务发现模型，经过实测，新的服务发现模型能实现节点到注册中心间数据传输量 90% 的下降，这就使得注册中心的压力极大降低，同时消费端的框架常驻内存也实现超 50% 下降。&lt;/p>
&lt;h3 id="压测数据">压测数据&lt;/h3>
&lt;p>下面是工商银行联合 Dubbo 社区给出的一组基于真实服务特点给出的模拟压测数据。&lt;/p>
&lt;p>&lt;img alt="icbc-data1" src="https://cn.dubbo.apache.org/imgs/user/icbc/icbc-data1.png">&lt;/p>
&lt;p>上图是对使用了应用级服务发现的消费端进程采样的内存对比数据。其中横轴是不同的 Dubbo 版本，纵轴是实际采样到的内存表现，可以看到 Dubbo 2.6、2.7 版本表现几乎一致，而升级到 3.0 版本后，即使不升级应用级服务发现，内存也降低接近 40%，而当切换到应用级服务发现之后，内存占用下降到只有原来的 30%。&lt;/p>
&lt;p>&lt;img alt="icbc-data2" src="https://cn.dubbo.apache.org/imgs/user/icbc/icbc-data2.png">&lt;/p>
&lt;p>上图是消费端的 GC 情况统计，同样的，横轴是不同的 Dubbo 版本，纵轴是实际采样到的 GC 表现。这里的压测数据，是通过模拟注册中心不停的往消费端进程推送地址列表的场景得到的。可以看到 Dubbo 2.6、2.7 版本表现几乎一致，而在 3.0 版本中切换到应用级服务发现之后，GC 已经趋近于零次。&lt;/p></description></item><item><title>饿了么全站成功升级 Dubbo3</title><link>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E9%A5%BF%E4%BA%86%E4%B9%88%E5%85%A8%E7%AB%99%E6%88%90%E5%8A%9F%E5%8D%87%E7%BA%A7-dubbo3/</link><pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate><guid>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E9%A5%BF%E4%BA%86%E4%B9%88%E5%85%A8%E7%AB%99%E6%88%90%E5%8A%9F%E5%8D%87%E7%BA%A7-dubbo3/</guid><description>&lt;h3 id="升级目标">升级目标&lt;/h3>
&lt;p>&lt;img alt="elem-arc" src="https://cn.dubbo.apache.org/imgs/user/eleme/elem-arc.png">&lt;/p>
&lt;p>这里是饿了么的的基本部署架构图。&lt;/p>
&lt;p>在升级之前，饿了么的微服务框架采用的是 HSF2，跨单元的 RPC 调用是通过 proxy 中转代理，在这个过程中 proxy 所承载的机器数和流量迅速增长，比较突出的一点是 proxy 在订阅所有的地址数据后资源消耗和稳定性都收到严峻挑战。&lt;/p>
&lt;p>通过全站升级 Dubbo3，业务线期望达到两个目标：&lt;/p>
&lt;ul>
&lt;li>将地址模型切换到应用级服务发现大幅减轻中心化节点和消费端节点的资源消耗压力。&lt;/li>
&lt;li>以应用级服务发现架构下的全局共享注册中心取代 proxy 模式，实现跨单元节点通信直连。&lt;/li>
&lt;/ul>
&lt;h3 id="升级过程">升级过程&lt;/h3>
&lt;p>&lt;img alt="eleme-upgrade1" src="https://cn.dubbo.apache.org/imgs/user/eleme/elem-upgrade1.png">&lt;/p>
&lt;p>不论是针对 Dubbo2 还是 HSF2，我们都做了全面的 API 兼容，因此 Dubbo3 基本可以做到零改造升级，并且每个应用都是独立透明升级，不需要关心它的上下游应用的升级状态，因为 Dubbo3 升级之后不论是从地址发现模型还是协议的默认行为都保持与 2.0 版本兼容，用户可以在任意时间点对任意应用按需切换 Dubbo3 行为。
如右图所示，我们模拟展示了饿了么集群 Dubbo3 升级过程的一个中间状态，其中灰色标记的是老版本 HSF2 应用，橙色和绿色标记的是已经升级 Dubbo3 的应用，橙色部分的应用及其调用链路代表不但已经升级到 Dubbo3，同时也完成了 Dubbo3 行为的切换，在这里是指已经切换到了应用级地址模型。这里的升级过程主要为了说明 Dubbo3 框架升级的兼容性和独立性。&lt;/p>
&lt;p>接下来，我们详细分析一下橙色部分节点往 Dubbo3 应用级发现迁移的具体过程。&lt;/p>
&lt;p>&lt;img alt="elem-upgrade-provider" src="https://cn.dubbo.apache.org/imgs/user/eleme/elem-upgrade-provider.png">&lt;/p>
&lt;p>首先看 Provider 侧，服务提供者在升级 Dubbo3 后会默认保持双注册行为，即同时注册接口级地址和应用级地址到注册中心，一方面保持兼容，另一方面为未来消费端迁移做好准备。双注册的开关可通过 -Ddubbo.application.register-mode=al/interface/interface控制，我们推荐保持双注册的默认行为以减少后续迁移成本。&lt;/p>
&lt;p>大家可能担心双注册给注册中心带来的存储压力，实际上在应用级服务发现模型下这并不是一个问题，因为大家如果回想前面我们对应用级服务发现工作原理的分析，注册地址已经被大幅精简，根据我们实际推算，每多注册一条应用级服务发现 URL 地址，只会增加 0.1% ～ 1% 的额外开销。&lt;/p>
&lt;p>&lt;img alt="elem-upgrade-consumer" src="https://cn.dubbo.apache.org/imgs/user/eleme/elem-upgrade-consumer.png">&lt;/p>
&lt;p>与提供端类似，要实现平滑迁移消费端也要经历双订阅的过程，流程上就不再赘述。消费端的双订阅行为也可通过规则或开关进行动态调整，控制消费端的消费的某个服务、应用迁移到应用级地址模型；除此之外，Dubbo3 还内置了自动决策机制，在发现应用级地址可用的情况下即会自动完成切换，并且这个行为是默认的。&lt;/p>
&lt;p>以下是消费端双订阅时的选址流程：&lt;/p>
&lt;p>&lt;img alt="elem-upgrade-consumer1" src="https://cn.dubbo.apache.org/imgs/user/eleme/elem-upgrade-consumer1.png">&lt;/p>
&lt;h3 id="升级效果">升级效果&lt;/h3>
&lt;p>&lt;img alt="elem-result" src="https://cn.dubbo.apache.org/imgs/user/eleme/elem-result.png">&lt;/p></description></item><item><title>店小蜜升级 Triple 协议</title><link>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E5%BA%97%E5%B0%8F%E8%9C%9C%E5%8D%87%E7%BA%A7-triple-%E5%8D%8F%E8%AE%AE/</link><pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate><guid>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E5%BA%97%E5%B0%8F%E8%9C%9C%E5%8D%87%E7%BA%A7-triple-%E5%8D%8F%E8%AE%AE/</guid><description>&lt;h1 id="前言">前言&lt;/h1>
&lt;p>阿里云-达摩院-云小蜜对话机器人产品基于深度机器学习技术、自然语言理解技术和对话管理技术，为企业提供多引擎、多渠道、多模态的对话机器人服务。17年云小蜜对话机器人在公共云开始公测，同期在混合云场景也不断拓展。为了同时保证公共云、混合云发版效率和稳定性，权衡再三我们采用了1-2个月一个大版本迭代。
经过几年发展，为了更好支撑业务发展，架构升级、重构总是一个绕不过去的坎，为了保证稳定性每次公共云发版研发同学都要做两件事：&lt;/p>
&lt;pre>&lt;code>1. 梳理各个模块相较线上版本接口依赖变化情况，决定十几个应用的上线顺序、每批次发布比例；
2. 模拟演练上述1产出的发布顺序，保证后端服务平滑升级，客户无感知；
&lt;/code>&lt;/pre>
&lt;p>上述 1、2 动作每次都需要 2-3 周左右的时间梳理、集中演练，但是也只能保证开放的PaaS API平滑更新；&lt;/p>
&lt;p>控制台服务因为需要前端、API、后端保持版本一致才能做到体验无损（如果每次迭代统一升级API版本开发、协同成本又会非常大），权衡之下之前都是流量低谷期上线，尽量缩短发布时间，避免部分控制台模块偶发报错带来业务问题。针对上面问题，很早之前就考虑过用蓝绿发布、灰度等手段解决，但是无奈之前对话机器人在阿里云内部业务区域，该不再允许普通云产品扩容，没有冗余的机器，流量治理完全没法做。&lt;/p>
&lt;h1 id="迁移阿里云云上">迁移阿里云云上&lt;/h1>
&lt;p>带着上面的问题，终于迎来的 2021 年 9 月份，云小蜜将业务迁移至阿里云云上。&lt;/p>
&lt;h2 id="dubbo3-的实践">Dubbo3 的实践&lt;/h2>
&lt;p>“当时印象最深的就是这张图，虽然当时不知道中间件团队具体要做什么事情，但是记住了两个关键词：三位一体、红利。没想到在2021年底，真真切切享受到了这个红利。”&lt;/p>
&lt;p>&lt;img alt="image1" src="https://cn.dubbo.apache.org/imgs/v3/users/yunxiaomi-1.png">&lt;/p>
&lt;p>云小蜜使用的是集团内部的HSF服务框架，需要迁移至阿里云云上，并且存在阿里云云上与阿里内部业务域的互通、互相治理的诉求。云小蜜的公共服务部署在公有云VPC，部分依赖的数据服务部署在内部，内部与云上服务存在RPC互调的诉求，其实属于混合云的典型场景。
简单整理了下他们的核心诉求，概括起来有以下三点吧：希望尽可能采用开源方案，方便后续业务推广；在网络通信层面需要保障安全性；对于业务升级改造来说需要做到低成本。&lt;/p>
&lt;p>&lt;img alt="image2" src="https://cn.dubbo.apache.org/imgs/v3/users/yunxiaomi-2.png">&lt;/p>
&lt;p>在此场景下，经过许多讨论与探索，方案也敲定了下来&lt;/p>
&lt;ul>
&lt;li>全链路升级至开源 Dubbo3.0，云原生网关默认支持Dubbo3.0，实现透明转发，网关转发RT小于1ms&lt;/li>
&lt;li>利用 Dubbo3.0 支持HTTP2特性，云原生网关之间采用 mTLS 保障安全&lt;/li>
&lt;li>利用云原生网关默认支持多种注册中心的能力，实现跨域服务发现对用户透明，业务侧无需做任何额外改动&lt;/li>
&lt;li>业务侧升级SDK到支持 Dubbo3.0，配置发布 Triple 服务即可，无需额外改动&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>解决了互通、服务注册发现的问题之后，就是开始看如何进行服务治理方案了&lt;/strong>&lt;/p>
&lt;h1 id="阿里云云上流量治理">阿里云云上流量治理&lt;/h1>
&lt;p>迁移至阿里云云上之后，流量控制方案有非常多，比如集团内部的全链路方案、集团内单元化方案等等。&lt;/p>
&lt;h2 id="设计目标和原则">设计目标和原则&lt;/h2>
&lt;ol>
&lt;li>要引入一套流量隔离方案，上线过程中，新、旧两个版本服务同时存在时，流量能保证在同一个版本的“集群”里流转，这样就能解决重构带来的内部接口不兼容问题。&lt;/li>
&lt;li>要解决上线过程中控制台的平滑性问题，避免前端、后端、API更新不一致带来的问题。&lt;/li>
&lt;li>无上线需求的应用，可以不参与上线。&lt;/li>
&lt;li>资源消耗要尽量少，毕竟做产品最终还是要考虑成本和利润。&lt;/li>
&lt;/ol>
&lt;h2 id="方案选型">方案选型&lt;/h2>
&lt;ol>
&lt;li>集团内部的全链路方案：目前不支持阿里云云上&lt;/li>
&lt;li>集团内单元化方案：主要解决业务规模、容灾等问题，和我们碰到的问题不一样&lt;/li>
&lt;li>搭建独立集群，版本迭代时切集群：成本太大&lt;/li>
&lt;li>自建：在同一个集群隔离新、老服务，保证同一个用户的流量只在同版本服务内流转&lt;/li>
&lt;/ol>
&lt;p>以RPC为例：&lt;/p>
&lt;ul>
&lt;li>方案一：通过开发保证，当接口不兼容升级时，强制要求升级HSF version，并行提供两个版本的服务； 缺点是一个服务变更，关联使用方都要变更，协同成本特别大，并且为了保持平滑，新老接口要同时提供服务，维护成本也比较高&lt;/li>
&lt;li>方案二：给服务（机器）按版本打标，通过RPC框架的路由规则，保证流量优先在同版本内流转&lt;/li>
&lt;/ul>
&lt;h2 id="全链路灰度方案">全链路灰度方案&lt;/h2>
&lt;p>就当1、2、3、4都觉得不完美，一边调研一边准备自建方案5的时候，兜兜绕绕拿到了阿里云 MSE 微服务治理团队&lt;a href="https://yuque.antfin.com/docs/share/a8df43ac-3a3b-4af4-a443-472828884a5d?">《20分钟获得同款企业级全链路灰度能力》&lt;/a>，方案中思路和准备自建的思路完全一致，也是利用了RPC框架的路由策略实现的流量治理，并且实现了产品化（微服务引擎-微服务治理中心），同时，聊了两次后得到几个“支持”，以及几个“后续可以支持”后，好像很多事情变得简单了&amp;hellip;&lt;/p>
&lt;p>&lt;img alt="image3" src="https://cn.dubbo.apache.org/imgs/v3/users/yunxiaomi-3.png">&lt;/p>
&lt;p>从上图可以看到，各个应用均需要搭建基线(base)环境和灰度(gray)环境，除了流量入口-业务网关以外，下游各个业务模块按需部署灰度（gray）环境，如果某次上线某模块没有变更则无需部署。&lt;/p>
&lt;h3 id="各个中间件的治理方案">各个中间件的治理方案&lt;/h3>
&lt;ol>
&lt;li>Mysql、ElasticSearch：持久化或半持久化数据，由业务模块自身保证数据结构兼容升级；&lt;/li>
&lt;li>Redis：由于对话产品会有多轮问答场景，问答上下文是在Redis里，如果不兼容则上线会导致会话过程中的C端用户受影响，因此目前Redis由业务模块自身保证数据结构兼容升级；&lt;/li>
&lt;li>配置中心：基线(base)环境、灰度(gray)环境维护两套全量配置会带来较大工作量，为了避免人工保证数据一致性成本，基线(base)环境监听dataId，灰度(gray)环境监听gray.dataId如果未配置gray.dataId则自动监听dataId；（云小蜜因为在18年做混合云项目为了保证混合云、公共云使用一套业务代码，建立了中间件适配层，本能力是在适配层实现）&lt;/li>
&lt;li>RPC服务：使用阿里云 one agent 基于Java Agent技术利用Dubbo框架的路由规则实现，无需修改业务代码；&lt;/li>
&lt;/ol>
&lt;p>应用只需要加一点配置：&lt;/p></description></item><item><title>瓜子二手车 Dubbo 实践</title><link>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E7%93%9C%E5%AD%90%E4%BA%8C%E6%89%8B%E8%BD%A6-dubbo-%E5%AE%9E%E8%B7%B5/</link><pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate><guid>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E7%93%9C%E5%AD%90%E4%BA%8C%E6%89%8B%E8%BD%A6-dubbo-%E5%AE%9E%E8%B7%B5/</guid><description>&lt;h2 id="前言">前言&lt;/h2>
&lt;p>  随着瓜子业务的不断发展，系统规模在逐渐扩大，目前在瓜子的私有云上已经运行着数百个dubbo应用，上千个dubbo实例。瓜子各部门业务迅速发展，版本没有来得及统一，各个部门都有自己的用法。随着第二机房的建设，dubbo版本统一的需求变得越发迫切。几个月前，公司发生了一次与dubbo相关的生产事故，成为了公司dubbo版本升级的诱因。&lt;/p>
&lt;p>  接下来，我会从这次事故开始，讲讲我们这段时间所做的dubbo版本升级的历程以及dubbo后续多机房的方案。&lt;/p>
&lt;h2 id="一ephermal节点未及时删除导致provider不能恢复注册的问题修复">一、Ephermal节点未及时删除导致provider不能恢复注册的问题修复&lt;/h2>
&lt;h3 id="事故背景">事故背景&lt;/h3>
&lt;p>  在生产环境，瓜子内部各业务线共用一套zookeeper集群作为dubbo的注册中心。2019年9月份，机房的一台交换机发生故障，导致zookeeper集群出现了几分钟的网络波动。在zookeeper集群恢复后，正常情况下dubbo的provider应该会很快重新注册到zookeeper上，但有一小部分的provider很长一段时间没有重新注册到zookeeper上，直到手动重启应用后才恢复注册。&lt;/p>
&lt;h3 id="排查过程">排查过程&lt;/h3>
&lt;p>  首先，我们统计了出现这种现象的dubbo服务的版本分布情况，发现在大多数的dubbo版本中都存在这种问题，且发生问题的服务比例相对较低，在github中我们也未找到相关问题的issues。因此，推断这是一个尚未修复的且在网络波动情况的场景下偶现的问题。&lt;/p>
&lt;p>  接着，我们便将出现问题的应用日志、zookeeper日志与dubbo代码逻辑进行相互印证。在应用日志中，应用重连zookeeper成功后provider立刻进行了重新注册，之后便没有任何日志打印。而在zookeeper日志中，注册节点被删除后，并没有重新创建注册节点。对应到dubbo的代码中，只有在&lt;code>FailbackRegistry.register(url)&lt;/code>的&lt;code>doRegister(url)&lt;/code>执行成功或线程被挂起的情况下，才能与日志中的情况相吻合。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-java" data-lang="java">&lt;span style="display:flex;">&lt;span> &lt;span style="color:#268bd2">public&lt;/span> &lt;span style="color:#dc322f">void&lt;/span> &lt;span style="color:#268bd2">register&lt;/span>(URL url) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#268bd2">super&lt;/span>.register(url);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> failedRegistered.remove(url);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> failedUnregistered.remove(url);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#719e07">try&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#586e75">// Sending a registration request to the server side&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> doRegister(url);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> } &lt;span style="color:#719e07">catch&lt;/span> (Exception e) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Throwable t &lt;span style="color:#719e07">=&lt;/span> e;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#586e75">// If the startup detection is opened, the Exception is thrown directly.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#dc322f">boolean&lt;/span> check &lt;span style="color:#719e07">=&lt;/span> getUrl().getParameter(Constants.CHECK_KEY, &lt;span style="color:#cb4b16">true&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#719e07">&amp;amp;&amp;amp;&lt;/span> url.getParameter(Constants.CHECK_KEY, &lt;span style="color:#cb4b16">true&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#719e07">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#719e07">!&lt;/span>Constants.CONSUMER_PROTOCOL.equals(url.getProtocol());
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#dc322f">boolean&lt;/span> skipFailback &lt;span style="color:#719e07">=&lt;/span> t &lt;span style="color:#719e07">instanceof&lt;/span> SkipFailbackWrapperException;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#719e07">if&lt;/span> (check &lt;span style="color:#719e07">||&lt;/span> skipFailback) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#719e07">if&lt;/span> (skipFailback) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> t &lt;span style="color:#719e07">=&lt;/span> t.getCause();
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#719e07">throw&lt;/span> &lt;span style="color:#719e07">new&lt;/span> IllegalStateException(&lt;span style="color:#2aa198">&amp;#34;Failed to register &amp;#34;&lt;/span> &lt;span style="color:#719e07">+&lt;/span> url &lt;span style="color:#719e07">+&lt;/span> &lt;span style="color:#2aa198">&amp;#34; to registry &amp;#34;&lt;/span> &lt;span style="color:#719e07">+&lt;/span> getUrl().getAddress() &lt;span style="color:#719e07">+&lt;/span> &lt;span style="color:#2aa198">&amp;#34;, cause: &amp;#34;&lt;/span> &lt;span style="color:#719e07">+&lt;/span> t.getMessage(), t);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> } &lt;span style="color:#719e07">else&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> logger.error(&lt;span style="color:#2aa198">&amp;#34;Failed to register &amp;#34;&lt;/span> &lt;span style="color:#719e07">+&lt;/span> url &lt;span style="color:#719e07">+&lt;/span> &lt;span style="color:#2aa198">&amp;#34;, waiting for retry, cause: &amp;#34;&lt;/span> &lt;span style="color:#719e07">+&lt;/span> t.getMessage(), t);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#586e75">// Record a failed registration request to a failed list, retry regularly&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> failedRegistered.add(url);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>  在继续排查问题前，我们先普及下这些概念：dubbo默认使用curator作为zookeeper的客户端，curator与zookeeper是通过session维持连接的。当curator重连zookeeper时，若session未过期，则继续使用原session进行连接；若session已过期，则创建新session重新连接。而ephemeral节点与session是绑定的关系，在session过期后，会删除此session下的ephemeral节点。&lt;/p></description></item><item><title>小米与 Dubbo 社区的合作</title><link>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E5%B0%8F%E7%B1%B3%E4%B8%8E-dubbo-%E7%A4%BE%E5%8C%BA%E7%9A%84%E5%90%88%E4%BD%9C/</link><pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate><guid>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E5%B0%8F%E7%B1%B3%E4%B8%8E-dubbo-%E7%A4%BE%E5%8C%BA%E7%9A%84%E5%90%88%E4%BD%9C/</guid><description>&lt;p>小米一直在积极拥抱开源社区并提交贡献，参与Dubbo3建设发布以来，在内部业务也积极推进升级工作，目前实例数已经升级到了一定的比例 。升级过程总体平稳，稳定性指标正常，性能提升明显，率先升级完成的应用更早拥有了mesh化的条件。从升级后的数据表现来看，Dubbo3改变以往接口粒度的注册发现方式为应用粒度的注册发现方式，这样带来了注册中心存储和运行的更稳定，降低运维成本；使用protobuf协议进行序列化与反序列化，性能和字节大小均提升数量级；完全兼容gprc，给小米这样多言语并存的服务环境带来了极大便利。&lt;/p>
&lt;p>Xiaomi is devoted to making continuous contribution to the open source community. Since the introduction of Dubbo3, internal projects are rapidly upgrading to the latest version of Dubbo. Currently, At present the numbers of instances has been upgrade to a certain proportion. Not only performance improvements have been seen, but services are also running smoothly with improvements in availability. Statistics provide proof that Dubbo’s switch from api-level discovery to application-level discovery has improved the availability and reliability of service discovery, which leads to lower operations cost. In addition, using ProtoBuf for serialization and deserialization has reduced data exchange size. Lastly, full compatibility with grpc provides convenience to Xiaomi’s multi-language development environment.&lt;/p></description></item><item><title>中伦网络 Dubbo3 升级实践</title><link>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E4%B8%AD%E4%BC%A6%E7%BD%91%E7%BB%9C-dubbo3-%E5%8D%87%E7%BA%A7%E5%AE%9E%E8%B7%B5/</link><pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate><guid>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E4%B8%AD%E4%BC%A6%E7%BD%91%E7%BB%9C-dubbo3-%E5%8D%87%E7%BA%A7%E5%AE%9E%E8%B7%B5/</guid><description>&lt;p>中伦网络在 2022 年完成了服务框架从 Dubbo2 到 Dubbo3 的全站升级，深度使用了应用级服务发现、Kubernetes 原生服务部署、服务治理等核心能力。来自中仑网络的技术负责人来彬彬对整个 Dubbo3 的选型、升级过程及收益等做了深入总结。&lt;/p>
&lt;p>值得一提的是近期 &lt;a href="https://cn.dubbo.apache.org/zh-cn/">Dubbo3 官网文档&lt;/a> 整体有了本质的提升，并且社区承诺短期内文档还会投入大量精力完善文档，这点对于 Dubbo3 的使用和用户信心提升非常重要。&lt;/p>
&lt;h3 id="一公司业务与技术架构简介">一、公司业务与技术架构简介&lt;/h3>
&lt;p>&lt;a href="https://www.zhonglunnet.com/guanyu.html">苏州中仑网络科技有限公司&lt;/a>是一家“专注零售门店增收服务”的公司，一直以“解决中小零售门店经营难的问题”为初心，致力于为零售商户提供门店运营一体化解决方案，帮助零售门店实现增收。中仑网络以零售技术为核心，为零售商户打造出集收银系统、中仑掌柜、微商城、汇邻生活平台、大数据平台、移动支付、智慧农贸、汇邻门店运营服务等为一体的新零售生态体系，实现线上线下全方位融合，为零售商家赋能增收。技术团队在构建之初选取Dubbo 2.5.3+Zookeeper版本构建公司微服务基座支撑公司业务发展，后期同阿里云深度合作整体迁移使用阿里云，使用云原生基础设施ACK（Kubernetes）+MSE（Zookeeper）+Dubbo+PolarDB等构建，实现可动态缩扩容的服务能力。
伴随合作商扩展3000+，市场遍及300+城市，零售商户30万+，服务覆盖餐饮、茶饮、服装、母婴、烘焙、生鲜、商超、美业、美妆、宠物等多个行业。伴随着领域拓宽、商户量快速增长上升，系统数量和部署节点也迎来了暴增，随之在系统可用性上受到较大挑战：微服务治理能力、微服务地址注册发现，Kubernetes平台服务的无损上下线顺滑度上问题与挑战越来越多。架构图见图一。&lt;/p>
&lt;p>&lt;img alt="image1" src="https://cn.dubbo.apache.org/imgs/v3/users/zhonglunwangluo-1.png">&lt;/p>
&lt;p>图一&lt;/p>
&lt;h3 id="二dubbo3-升级总结">二、Dubbo3 升级总结&lt;/h3>
&lt;p>在升级微服务组件技术选型上主要考虑解决以前的痛点：服务治理能力、云原生友好性、服务注册发现，这几个制约业务发展的紧要问题。比较下来Dubbo3架构设计理念与我们较为契合，能较好的满足我们业务发展要求。&lt;/p>
&lt;h3 id="1服务治理能力">1、服务治理能力&lt;/h3>
&lt;p>Dubbo 3提供丰富的服务治理能力，可实现诸如服务发现、负载均衡、流量调度等服务治理诉求。在使用上我们有两种选择：一、使用Dubbo管理控制台管理配置、二、集成相关API能力到系统。同时Dubbo 扩展性较好，可以在很多功能点（见图二）去定制自己的实现，以改变框架的默认行为来满足自己的业务需求。Dubbo SPI ( Service Provider Interface)将接口实现类的全限定名配置在文件中，并由服务加载器读取配置文件，加载实现类。这样可以在运行时，动态为接口替换实现类。基于此特性，我们可以很容易的通过 SPI 机制为我们的程序提供拓展功能，如我们在此基础上实现了基于生产和消费者过滤器Filter实现全链路自定义的链路监控;基于路由扩展标签路由方式进行测试环境的隔离方便快速多版本服务测试验证。实操上我们基于生产者注册服务时打标，如原系统A V1版本部署在fat环境上,现在为了测试V2版本，我们将V2版本打标tag=fat-v2;使用端在消费时指定Invocation Attachment 参数,inv.setAttachment(TAG_KEY, routeTag);基于此我们可以方便自测试，同时生产上我们也可以做简单的生产灰度运用。&lt;/p>
&lt;p>&lt;img alt="image2" src="https://cn.dubbo.apache.org/imgs/v3/users/zhonglunwangluo-2.png">
图二&lt;/p>
&lt;h3 id="2云原生友好性">2、云原生友好性&lt;/h3>
&lt;p>Dubbo 在设计上遵循云原生微服务开发理念，微服务支持 Kubernetes平台调度，实现服务生命周期与容器生命周期的对齐，包括 Dubbo 的启动、销毁、服务注册等生命周期事件。中仑网络微服务管理使用的是MSE（Zookeeper）,因而我们服务暴露使用需与之对齐。具体操作上我们自定义Startup 启动探针、 Liveness 存活探针、Readiness 就绪探针。项目的正常切换需要保障无损的上下线，在实施中无损上线相对于下线来说会更麻烦点，项目的发布上线过程大体会遵从如下流程：大致分成三个阶段，第一阶段升级少量（如 20% ）的实例，并切换少量流量到新版本，完成这个阶段后先暂停升级。经过人工确认之后继续第二个阶段，升级更大比例（如 90% ）的实例和流量，再次暂停等待人工确认。最后阶段将全量升级到新版本并验证完毕，从而完成整个发布过程。如果升级期间发现包括业务指标在内的任何异常，例如 CPU 或 memory 异常使用率升高或请求 500 日志过多等情况，可以快速回滚。因为我们使用的是MSE（Zookeeper）服务，dubbo服务自注册在应用启动过程暴露不受Kubernetes 生命周期的控制，出现项目未完全就绪部分服务可被提前可被访问问题。&lt;/p>
&lt;p>&lt;img alt="image3" src="https://cn.dubbo.apache.org/imgs/v3/users/zhonglunwangluo-3.png">
图三&lt;/p>
&lt;p>实施处理上我们主要利用Dubbo Qos指令,初始使用服务不暴露，在应用就绪后调用Qos online指令进行服务上线替换老节点，每次替换的节点数量基于发布策略来制定;下线过程针对需下线节点我们会先使用Qos指令进行下线offline操作等待应用执行完服务，从而进行优雅停机，从实践的效果来看能满足我们的生产需求。&lt;/p>
&lt;h3 id="3实例级别升级切换">3、实例级别升级切换&lt;/h3>
&lt;p>相比于 2.x 版本中的基于接口粒度的服务发现机制，3.x 引入了全新的基于应用粒度的服务发现机制，进一步提升了 Dubbo3 在大规模集群实践中的性能与稳定性。此次升级过程中我们也同步引入了配置中心与原数据中心，即将图四置灰部分启用&lt;/p></description></item><item><title>平安健康的 Dubbo3 迁移历程</title><link>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E5%B9%B3%E5%AE%89%E5%81%A5%E5%BA%B7%E7%9A%84-dubbo3-%E8%BF%81%E7%A7%BB%E5%8E%86%E7%A8%8B/</link><pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate><guid>https://cn.dubbo.apache.org/zh-cn/blog/2023/01/15/%E5%B9%B3%E5%AE%89%E5%81%A5%E5%BA%B7%E7%9A%84-dubbo3-%E8%BF%81%E7%A7%BB%E5%8E%86%E7%A8%8B/</guid><description>&lt;h1 id="1-背景">1 背景&lt;/h1>
&lt;p>我们公司从15年开始就使⽤dubbo作为微服务框架，当社区推出dubbo3时，我们也⽴刻跟进并做了深⼊调研，发现dubbo3 的应⽤/实例级服务注册和发现模式能够在一定程度上解决我们当前注册中⼼⾯临的压⼒，解决稳定性和安全性问题。同时dubbo3在服务治理上也做了升级，契合云原⽣架构，⽽且dubbo3能够向下兼容dubbo2，这也将降低升级的成本和⻛险。&lt;/p>
&lt;p>升级项目有了阶段性的进展，目前仍然在进行中。通过本⽂，我们对公司内部的Dubbo3 升级过程及收益等做了深⼊总结。&lt;/p>
&lt;h1 id="2-dubbo3-核功能介绍">2 Dubbo3 核⼼功能介绍&lt;/h1>
&lt;p>dubbo社区关于dubbo3的文档和资料越来越完善，以下是我们从社区引用的一些内容。&lt;/p>
&lt;h2 id="21-下一代云原生服务框架">2.1 下一代云原生服务框架&lt;/h2>
&lt;p>&lt;img alt="pingan" src="https://cn.dubbo.apache.org/imgs/v3/users/pingan-2.png">&lt;/p>
&lt;p>Dubbo3被社区寄予厚望，将其视为下一代云原生服务框架打造，Dubbo3 提供的核心特性列表，主要包括四部分。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>全新服务发现模型&lt;/strong> 。应用粒度服务发现，面向云原生设计，适配基础设施与异构系统；性能与集群伸缩性大幅提升。&lt;/li>
&lt;li>**下一代 **** RPC **&lt;strong>协议&lt;/strong> &lt;strong>Triple&lt;/strong> 。基于 HTTP/2 的 Triple 协议，兼容 gRPC；网关穿透性强、多语言友好、支持 Reactive Stream。&lt;/li>
&lt;li>&lt;strong>统一流量治理模型&lt;/strong> 。面向云原生流量治理，SDK、Mesh、VM、Container 等统一治理规则；能够支持更丰富的流量治理场景。&lt;/li>
&lt;li>&lt;strong>Service Mesh&lt;/strong> 。在最新的3.1.0的版本中支持Sidecar Mesh 与 Proxyless Mesh，提供更多架构选择，降低迁移、落地成本。&lt;/li>
&lt;/ul>
&lt;p>&lt;img alt="pingan" src="https://cn.dubbo.apache.org/imgs/v3/users/pingan-3.png">&lt;/p>
&lt;p>首先是性能、资源利用率的提升。社区资料显示，升级 Dubbo3 的应用预期能实现单机内存 50% 的下降，对于越大规模的集群效果将越明显，Dubbo3 从架构上支持百万实例级别的集群横向扩展,同时依赖应用级服务发现、Triple协议等可以大大提供应用的服务治理效率和吞吐量。&lt;/p>
&lt;p>其次， Dubbo3 让业务架构升级变得更容易、更合理，尤其是RPC协议，在 2.x 版本中，web、移动端与后端的通信都要经过网关代理，完成协议转换、类型映射等工作，dubbo3的Triple 协议让这些变得更容易与自然；并通过流式通信模型满足更多的使用场景。&lt;/p>
&lt;p>最后，得益于 Dubbo3 的完善云原生解决方案，Dubbo3的mesh架构可以帮助业务屏蔽底层云原生基础设施细节，让业务更专注于业务，这也是mesh的最根本的优势。&lt;/p>
&lt;h2 id="22-应用级服务发现核心原理">2.2 应用级服务发现核心原理&lt;/h2>
&lt;p>&lt;img alt="pingan" src="https://cn.dubbo.apache.org/imgs/v3/users/pingan-4.png">&lt;/p>
&lt;p>我们从 Dubbo 最经典的工作原理图说起，Dubbo 从设计之初就内置了服务地址发现的能力，Provider 注册地址到注册中心，Consumer 通过订阅实时获取注册中心的地址更新，在收到地址列表后，consumer 基于特定的负载均衡策略发起对 provider 的 RPC 调用。 在这个过程中&lt;/p></description></item><item><title>全国首个政企采购云平台：政采云的混合云跨网方案实践</title><link>https://cn.dubbo.apache.org/zh-cn/blog/2023/03/22/%E5%85%A8%E5%9B%BD%E9%A6%96%E4%B8%AA%E6%94%BF%E4%BC%81%E9%87%87%E8%B4%AD%E4%BA%91%E5%B9%B3%E5%8F%B0%E6%94%BF%E9%87%87%E4%BA%91%E7%9A%84%E6%B7%B7%E5%90%88%E4%BA%91%E8%B7%A8%E7%BD%91%E6%96%B9%E6%A1%88%E5%AE%9E%E8%B7%B5/</link><pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate><guid>https://cn.dubbo.apache.org/zh-cn/blog/2023/03/22/%E5%85%A8%E5%9B%BD%E9%A6%96%E4%B8%AA%E6%94%BF%E4%BC%81%E9%87%87%E8%B4%AD%E4%BA%91%E5%B9%B3%E5%8F%B0%E6%94%BF%E9%87%87%E4%BA%91%E7%9A%84%E6%B7%B7%E5%90%88%E4%BA%91%E8%B7%A8%E7%BD%91%E6%96%B9%E6%A1%88%E5%AE%9E%E8%B7%B5/</guid><description>&lt;p>对云岛业务结构的公司来说，云平台属于公司内部、完全可控的局域网，而岛端则是有自己安全网络策略的独立内部网络。需要云岛通信时，会基于需求，按客户要求走流程开通一些端口，这个过程需要一定的成本且不完全可控。业务上，如果这种跨网需求增多，则会逐渐变成痛点。如果可以搭建一个透明的跨网传输网络，配合良好的顶层设计，就可以在业务支撑、安全管控和运维成本中寻求较好的平衡。&lt;/p>
&lt;p>本文将介绍政采云基于 Dubbo 的跨网方案落地过程中面临的技术挑战、社区合作以及更深层次抽象的一些思考。在政采云这种政企业务场景中的数据跨网，与业界公有云、自建私有云的公司相比，既有共性又有自己的特点，希望能为大家提供新的思路或者启发。&lt;/p>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>稳定、高效、可靠的基础设施是互联网企业应对业务高峰流量的底层基石。作为政采云的基础技术平台，基础平台部一直致力于通过业内前沿技术的落地，保障公司内部所有业务在线生产系统所依赖的基础技术平台能稳定、安全、低成本、可持续地运行与发展。&lt;/p>
&lt;p>由于公司对 Dubbo 框架的重度使用，&lt;strong>跨网数据传输系统&lt;/strong>一般基于 Dubbo 特性开发，在政采云内部就有多个版本的实现。&lt;/p>
&lt;p>早在几年前，政采云就上线了基于 Dubbo Filter 转发的方案，它解决了岛到云的单向数据传输，安全认证等问题。另外，业务部门也有按照自己的需求，推出网状点对点的方案，实现了一定程度的透明传输。&lt;/p>
&lt;p>结合前两年的探索实践以及业界相关领域技术的成熟度，2022年下半年，我们对各跨岛方案，进行了整合升级，也就是现在的&lt;strong>高速公路&lt;/strong>方案，保障跨岛标准化同时，解决了之前方案实践过程中面临的很多业务痛点，包括：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>单向传输&lt;/strong>：因为架构原因，如需双向需要对等重新部署一套，成本较大。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>白名单开通成本高&lt;/strong>：点对点的网状架构，需要两两开通白名单，因为政企网络特殊性，开通流程复杂且慢。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>平台维护成本高&lt;/strong>：业务各自一套数据传输平台，重复建设且运维成本高。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>公共功能的缺失&lt;/strong>：核心功能，业务可以按需开发，但是数据审计、链路追踪、可观测性等公共特性，往往没有足够投入。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="1-跨网数据传输系统演进">1. 跨网数据传输系统演进&lt;/h2>
&lt;h3 id="11-历史架构">1.1 历史架构&lt;/h3>
&lt;p>&lt;img alt="img" src="https://cn.dubbo.apache.org/imgs/v3/users/zcy-1.png">&lt;/p>
&lt;p>​&lt;/p>
&lt;p>自左向右、自下而上进行模块介绍：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>业务Web&lt;/strong>：业务 Web 作为数据发送方，调本地	集群 Provider 时，携带跨岛信息过去（Dubbo 上下文）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>岛业务Center&lt;/strong>：本地虚拟Provider，通过Filter拦截跨岛请求，通过http传送到云平台 Dubbo 网关，返回数据后反序列化返回岛业务 web。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Dubbo网关&lt;/strong>：接收 Http 请求，通过泛化调用云端 Provider，处理数据后返回业务 Center。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>云业务Center&lt;/strong>：普通 Dubbo Provider。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="12-高速公路架构">1.2 高速公路架构&lt;/h3>
&lt;p>&lt;img alt="img" src="https://cn.dubbo.apache.org/imgs/v3/users/zcy-2.png">&lt;/p>
&lt;p>​&lt;/p>
&lt;p>&lt;strong>1.2.1 隧道机制&lt;/strong>&lt;/p>
&lt;p>隧道技术是一种通过使用&lt;strong>互联网络&lt;/strong>的&lt;strong>基础设施&lt;/strong>在网络之间传递数据的方式。使用隧道传递的&lt;strong>数据&lt;/strong>(或负载)可以是不同协议的数据帧或包。&lt;/p>
&lt;p>高速公路架构中，使用了隧道这个概念。两端（业务层）是 Dubbo 私有协议，跨网传输过程中，则使用了 http 协议，http 协议可以更好的被中间设备、网关识别转发。这个机制的最大便利在于对业务的低侵入性。对于业务集群的应用完全不需要修改。
&lt;img alt="img" src="https://cn.dubbo.apache.org/imgs/v3/users/zcy-3.png">&lt;/p>
&lt;p>除了路由标记，出口/入口 Dubbo 协议字节流没有任何业务外信息，所以可以路由任何 Dubbo 请求。&lt;/p>
&lt;p>&lt;img alt="img" src="https://cn.dubbo.apache.org/imgs/v3/users/zcy-4.png">&lt;/p>
&lt;p>&lt;strong>1.2.2 主要节点&lt;/strong>&lt;/p>
&lt;p>&lt;strong>客户端 Sdk&lt;/strong>：不改变用户使用 Dubbo 的方式，多种形式提供Dubbo的路由。&lt;/p>
&lt;p>**Dubbo 出口网关：**代理 Dubbo 流量出口。&lt;/p></description></item></channel></rss>